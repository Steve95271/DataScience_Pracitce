{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will work with the [Boston Housing](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) dataset that contains information collected by the U.S Census Service concerning housing in the area of Boston Massachusetts. It consists of the following numerical features:\n",
    "- CRIM: per capita crime rate by town \n",
    "- ZN: proportion of residential land zoned for lots over 25,000 sq.ft. \n",
    "- INDUS: proportion of non-retail business acres per town \n",
    "- CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) \n",
    "- NOX: nitric oxides concentration (parts per 10 million) \n",
    "- RM: average number of rooms per dwelling \n",
    "- AGE: proportion of owner-occupied units built prior to 1940 \n",
    "- DIS: weighted distances to five Boston employment centres \n",
    "- RAD: index of accessibility to radial highways \n",
    "- TAX: full-value property-tax rate per $10,000 \n",
    "- PTRATIO: pupil-teacher ratio by town \n",
    "- B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town \n",
    "- LSTAT: % lower status of the population \n",
    "- MEDV: Median value of owner-occupied homes in $1000's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** As usual, inspect the file *boston_housing.csv* and check if it contains column names and what character is used as a column separator. Create a dataframe from the file and print the first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:** Use the *info* method to get an overview of missing values and to check if all datatypes have been interpreted correctly. How many instances are in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** We notice that the *MEDV* variable has missing values. We can use the [dropna](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html) method to remove the corresponding instances as illustrated in the following code cell. Call *info* again in the next code cell and check the number of instances in the dataset and the number of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4:** Compute numerical statistics for the variables to get an overview of their values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5:** We are interested in predicting the median value of homes (MEDV). Use a for-loop to create scatterplots for the relationship between MEDV and each other variable. The [columns](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.columns.html) attribute of dataframes is useful for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:** Based on the scatterplots, we may suspect that MEDV depends linearly and positively on RM. There seems to be also a negative monotonic relationship to LSTAT. To understand this better, compute the Pearson, Spearman, and Kendall correlation matrix for the dataframe and interpret the values for the aforementioned variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:** The Pearson correlation between RM and MEDV is 0.74. While this is not huge, along with the scatterplot, it indicates that there is a linear relationship. Since the Spearman correlation is even lower, there is no reason to believe that there is a stronger monotonic relationship. For LSTAT, in contrast, we note that the Spearman correlation is indeed significantly larger. This is consistent with the scatterplot and indicates that there is a negative non-linear monotonic  relationship. \n",
    "\n",
    "Given the linear relationship between RM and MEDV, RM alone may be a relatively good predictor for MEDV. Create a matrix *X* for the RM column and a target vector *y* for the MEDV column as explained in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = \n",
    "y = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7:** In order to get a reasonable estimate for the performance of our model on unseen data, we should separate the data into training/validation and test data. The following code cell demonstrates how SKLearn's [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function can be used for this purpose. To make our experiments reproducible, it is important to configure randomized methods properly. We can do this by means of the **random_state** parameter. The easiest way to make our results reproducible is to set it to a fixed number (that will be used as the seed for the random number generator). The **test_size** parameter determines the size of the test set and can be defined as a percentage (float) or absolute number (int). We use 400 examples for training and leave 52 for testing.\n",
    "\n",
    "Use the next code cell, to compute a second split *X_train2, X_test2, y_train2, y_test2*, but change the random_state parameter to 2. Compare the first 5 rows of X, X_train and X_train2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=52, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:5], X_train[0:5], X_train2[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8:** Fit a linear regression model on X_train and y_train. Print the intercept and the regression coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9:** Compute MAE and MAPE on the training data and interpret them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10:** Compute MAE and MAPE on the test data. How do the test results compare to the train results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 11:** Repeat the previous exercise for *LSTAT*. That is,\n",
    "- Create the predictor matrix and the target vector\n",
    "- Create training and test data\n",
    "- Fit a linear regression model and print the most important information about the model\n",
    "- Compute MAE and MAPE on training and test data and discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 12:** LSTAT is a slightly worse linear predictor than RM as we should expect based on our correlation analysis. However, given the strong monotonic relationship between\n",
    "LSTAT and RM, we may be able to do better by engineering a non-linear feature of LSTAT. The relationship in the scatterplot resembles a negative logarithmic\n",
    "relationship. We therefore add a new logarithmic feature of LSTAT in the following code cell.\n",
    "\n",
    "Use the next code cells to visualize the relationship between *logLSTAT* and *MEDV* in a scatterplot. Do you expect higher or lower Pearson correlation now? Compute the Pearson and Spearman correlation to check your intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['logLSTAT'] = np.log(df['LSTAT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 13:** Repeat the ML cycle for *logLSTAT*. That is,\n",
    "- Create the predictor matrix and the target vector\n",
    "- Create training and test data\n",
    "- Fit a linear regression model and print the most important information about the model\n",
    "- Compute MAE and MAPE on training and test data and discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 14:** *logLSTAT* seems to be a better predictor that *LSTAT* and *RM*. However, the test MAPE is still quite large. Build a new linear regression model that uses both \n",
    "*logLSTAT* and *RM*. As usual,\n",
    "- Create the predictor matrix and the target vector\n",
    "- Create training and test data\n",
    "- Fit a linear regression model and print the most important information about the model\n",
    "- Compute MAE and MAPE on training and test data and discuss the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 15:** Our new model does better, but there is not a huge difference. We can expect the largest increase in performance, when the target variable both\n",
    "1. depends on the new predictor and\n",
    "2. the new predictor is independent of the existing predictors.\n",
    "\n",
    "To get an idea of how much new information *RM* adds to *logLSTAT*, visualize their relationship in a scatterplot and compute the Pearson correlation between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 16:** The scatterplot and the Pearson correlation indicate that there is a relatively strong linear relationship between the variables, which explains the rather moderate increase in performance. While scatterplots and correlation coefficients can be useful to find relationship between variables, it is important to note that they can easily miss more complicated relationships between more than two variables. To illustrate this, the following code cell initializes two variables X1 and X2 and a third variable that depends on both X1 and X2. Use the next code cell to plot a scatterplot, and to compute the Pearson, Spearman and Kendall correlation between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.random.normal(size=100)\n",
    "x2 = np.random.normal(size=100)\n",
    "y = (x1 - x2)**2\n",
    "\n",
    "df2 = pd.DataFrame({'X1': x1, 'X2': x2, 'Y': y})\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.plot.scatter(x='X2', y='Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.corr('pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.corr('spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.corr('kendall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 17:** Repeat the ML cycle for the housing dataset with all variables. That is,\n",
    "- Create the predictor matrix and the target vector\n",
    "- Create training and test data\n",
    "- Fit a linear regression model and print the most important information about the model\n",
    "- Compute MAE and MAPE on training and test data and discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 18:** Spend some additional time on analyzing the relationships between predictors and the target, and the relationships between the variables. \n",
    "Try to systematically find variables that you can remove from the model without hurting performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr('spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify predictors that are most strongly correlated with other predictors (sum up absolute value of correlation coefficients)\n",
    "df.corr('spearman')[df.columns[:-1]].abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime Evaluation of Pandas' Kendall Correlation Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 19:** We want to evaluate the runtime of computing the Kendall rank correlation matrix on our computer. The following code randomly generates dataframes with two variables of increasing size and stores the size and the runtime for computing the matrix in lists. Generating the data may take a minute.\n",
    "\n",
    " Use the next code cell to initialize a dataframe with the runtime data and print the last 10 instances of your dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "size = []\n",
    "runtime = []\n",
    "\n",
    "for N in [40000 * (i+1) for i in range(30)]: #consider datasets of increasing size\n",
    "\n",
    "    for trial in range(5): #generate several instances for each N to get an idea of the variance of the runtime\n",
    "\n",
    "        A = 100 * np.random.uniform(size = (N, 2))  #generate random data for 2 features and N instances\n",
    "        df = pd.DataFrame(A)\n",
    "\n",
    "        start = time.time()\n",
    "        df.corr('kendall')\n",
    "        end = time.time()\n",
    "\n",
    "        size.append(N)\n",
    "        runtime.append(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimeData = \n",
    "runtimeData.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 20:** Create a scatterplot to visualize the relationship between size and runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 21:** Compute the Pearson, Spearman and Kendall correlation between N and Runtime and discuss the result. Keep in mind that a linear/monotonic relationship guarantees a large magnitude of the coefficient, but observing a large magnitude does not necessarily mean that the relationship is linear/monotonic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 22:** Train a linear regression model to predict the runtime based on N. Plot the scatterplot with a regression line to illustrate the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 23:** Compute RMSE, MAE and MAPE on the training data. Discuss the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 24:** Since the Kendall correlation coefficient depends on all pairs of observations, there is reason to believe that the runtime is not linear. To build a potentially better model, create some additional features to the dataframe including\n",
    "- $N \\cdot \\log N$,\n",
    "- $N^2$,\n",
    "- $N^3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimeData['N log N'] = \n",
    "runtimeData['N**2'] = \n",
    "runtimeData['N**3'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 25:** Train additional linear regression models on\n",
    "- $N \\cdot \\log N$,\n",
    "- $N^2$,\n",
    "- $N^3$,\n",
    "- $N$ and $N \\cdot \\log N$,\n",
    "- $N$ and $N^2$,\n",
    "- all features.\n",
    "\n",
    "Remember that *df[ 'columnName' ]* will return a vector, while *df[ ['columnName'] ]* will return a matrix. The former has to be reshaped to a matrix because the linear regression model expects a predictor matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = runtimeData[['N log N']].to_numpy()\n",
    "X2 = runtimeData[['N**2']].to_numpy()\n",
    "X3 = runtimeData[['N**3']].to_numpy()\n",
    "X4 = runtimeData[['N', 'N log N']].to_numpy()\n",
    "X5 = runtimeData[['N', 'N**2']].to_numpy()\n",
    "X6 = runtimeData[['N', 'N log N', 'N**2', 'N**3']].to_numpy()\n",
    "\n",
    "lr1 = LinearRegression().fit(X1, y)\n",
    "lr2 = LinearRegression().fit(X2, y)\n",
    "lr3 = LinearRegression().fit(X3, y)\n",
    "lr4 = LinearRegression().fit(X4, y)\n",
    "lr5 = LinearRegression().fit(X5, y)\n",
    "lr6 = LinearRegression().fit(X6, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 26:** Compute MAE and MAPE for all models on the training data. Discuss the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 27:** Evaluate how well the model generalizes to unseen dataframes. To do so, generate a small test set with 20 new random dataframes with size ranging from 10,000 to 200,000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 28:** Compute MAE and MAPE for all models on the test set and discuss the results. How does the training error compare to the test error? Which model seems best overall? Which models would you trust based on your background knowledge and the empirical evidence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
